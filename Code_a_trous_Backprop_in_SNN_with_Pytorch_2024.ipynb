{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWTf7H60Z0i_"
   },
   "source": [
    "# Rétropropagation de l'erreur dans les réseaux de neurones à décharges\n",
    "### GEI723, Automne 2024\n",
    "Ce notebook présente comment l'algorithme de la descente du gradient peut être adapté pour la rétropropagation de l'erreur dans les réseaux de neurones à décharges avec des fonctions d'activation non dérivables.\n",
    "\n",
    "Ce notebook est utilisé dans le cadre du cours GEI723 (Neuro-Computationnel).\n",
    "\n",
    "- **Encodage de l'entrée** : Les entrées sont encodées sous forme de trains de spikes.\n",
    "- **Structure du réseau** : Réseau de neurones à décharges avec plusieurs couches, utilisant différentes fonctions d'activation\n",
    "- **Études menées** : \n",
    "  - Impact des fonctions d'activation et des dérivées sur la performance.\n",
    "  - Analyse des méta-paramètres (nombre de couches, taux d'apprentissage, taille des lots).\n",
    "  - Comparaison entre réseaux avec et sans apprentissage sur certaines couches.\n",
    "- **Objectif du code** : Optimisation et analyse de la rétropropagation de l'erreur des réseaux de neurones à décharges.\n",
    "\n",
    "**Auteurs :**\n",
    "Clémence Lamballe\n",
    "Behrouz Nik-Nejad-Kazem-Pour\n",
    "Jean-Sébastien Giroux\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook a été inspiré du notebook créé par Ismaël Balafrej, Ph.D. avec Jean Rouat, Ph.D., ing., professeur et adapté par Ahmad El Ferdaoussi, Ph.D. et Arnaud Yarga, étud. Ph.D, dont le copyright et les auteurs sont:\n",
    "\n",
    "Copyright (c) 2019-2024, Université de Sherbrooke, groupe de recherche NECOTIS. Tous droits réservés.  \n",
    "Auteurs: Ismael Balafrej, Jean Rouat, adapté par Ahmad El Ferdaoussi et Arnaud Yarga\n",
    "\n",
    "\n",
    "Ce travail a lui même été adapté et inspiré des articles suivants:\n",
    "1. Surrogate Gradient Learning in Spiking Neural Networks by Zenke & Ganguli (2018) https://arxiv.org/pdf/1901.09948.pdf\n",
    "2. SLAYER: Spike Layer Error Reassignment in Time (2018) https://arxiv.org/pdf/1810.08646.pdf\n",
    "3. Biologically inspired alternatives to backpropagation through time for learning in recurrent neural nets (2019) https://arxiv.org/pdf/1901.09049.pdf\n",
    "\n",
    "\n",
    "Dans cet exemple de code la gestion du potentiel et de l'intensité du neurone est placé dans la gestion des couches.\n",
    "Vous pouvez décider de le faire autrement, par exemple dans la phase de propagation avant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpKksf2Ik4ga"
   },
   "source": [
    "# Packages et imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IJGOX5B8F7ic"
   },
   "outputs": [],
   "source": [
    "%pip install quantities sparse\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, model_selection, utils\n",
    "import torch\n",
    "import quantities as units\n",
    "from sparse import COO\n",
    "import enum\n",
    "from torch.autograd import Function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Préparation de la configuration pour le réseau de neurones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BspwdQSOk8U-"
   },
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ngz0VkgVmJB6"
   },
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Use the GPU unless there is none available.\n",
    "# If you don't have a CUDA enabled GPU, I recommned using Google Colab,\n",
    "# available at https://colab.research.google.com. Create a new notebook\n",
    "# and then go to Runtime -> Change runtime type -> Hardware accelerator -> GPU\n",
    "# Colab gives you access to up to 12 free continuous hours of a fairly recent GPU.\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frOGRjz0mL_5"
   },
   "source": [
    "### Préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hp-KevLKmLLP"
   },
   "outputs": [],
   "source": [
    "# Let's download the MNIST dataset, available at https://www.openml.org/d/554\n",
    "# You can edit the argument data_home to the directory of your choice.\n",
    "# The dataset will be downloaded there; the default directory is ~/scikit_learn_data/\n",
    "X, y = datasets.fetch_openml('mnist_784', version=1, return_X_y=True, data_home=None, as_frame=False)\n",
    "nb_of_samples, nb_of_features = X.shape\n",
    "# X = 70k samples, 28*28 features; y = 70k samples, 1 label (string)\n",
    "\n",
    "# Shuffle the dataset\n",
    "X, y = utils.shuffle(X, y)\n",
    "\n",
    "# Convert the labels (string) to integers for convenience\n",
    "y = np.array(y, dtype=int)\n",
    "nb_of_ouputs = np.max(y) + 1\n",
    "\n",
    "# We'll normalize our input data in the range [0, 1[.\n",
    "X = X / pow(2, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dgamMat1mXvu"
   },
   "source": [
    "### Conversion en décharges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "moU3ZUh8mSFG",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# And convert the data to a spike train using TTFS encoding\n",
    "# Choix utilisateur : dt\n",
    "dt = 1*units.ms\n",
    "duration_per_image = 100*units.ms\n",
    "absolute_duration = int(duration_per_image / dt)\n",
    "\n",
    "time_of_spike = (1 - X) * absolute_duration  # The brighter the pixel, the earlier the spike\n",
    "time_of_spike[X < .25] = 0  # \"Remove\" the spikes associated with darker pixels, which presumably carry less information\n",
    "\n",
    "sample_id, neuron_idx = np.nonzero(time_of_spike)\n",
    "\n",
    "# We use a sparse COO array to store the spikes for memory requirements\n",
    "# You can use the spike_train variable as if it were a tensor of shape (nb_of_samples, nb_of_features, absolute_duration)\n",
    "spike_train = COO((sample_id, neuron_idx, time_of_spike[sample_id, neuron_idx].astype(int)),\n",
    "                  np.ones_like(sample_id), shape=(nb_of_samples, nb_of_features, absolute_duration))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAxIwqtumyda"
   },
   "source": [
    "### Split entrainement/test/Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre total d'échantillons\n",
    "print(\"nb_of_samples =\", nb_of_samples)\n",
    "\n",
    "# 80% pour l'entraînement\n",
    "nb_of_train_samples = int(nb_of_samples * 0.80)\n",
    "\n",
    "# 10% pour test\n",
    "nb_of_test_samples = int(nb_of_samples * 0.10)\n",
    "\n",
    "# 10% pour validation\n",
    "nb_of_validation_samples = nb_of_samples - nb_of_train_samples - nb_of_test_samples\n",
    "\n",
    "# Création des indices\n",
    "train_indices = np.arange(nb_of_train_samples)\n",
    "validation_indices = np.arange(nb_of_train_samples, nb_of_train_samples + nb_of_validation_samples)\n",
    "test_indices = np.arange(nb_of_train_samples + nb_of_validation_samples, nb_of_samples)\n",
    "\n",
    "\n",
    "print(\"\\ntrain_indices =\", train_indices)\n",
    "print(\"train_indices.shape =\", train_indices.shape)\n",
    "\n",
    "\n",
    "print(\"\\nvalidation_indices =\", validation_indices)\n",
    "print(\"validation_indices.shape =\", validation_indices.shape)\n",
    "\n",
    "print(\"\\ntest_indices =\", test_indices)\n",
    "print(\"test_indices.shape =\", test_indices.shape)\n",
    "\n",
    "\n",
    "total_samples = train_indices.shape[0] + test_indices.shape[0] + validation_indices.shape[0]\n",
    "print(\"\\ntotal =\", total_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZWlfjmhjmdPz"
   },
   "source": [
    "### Création du réseau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choix utilisateur : nombre de couches cachées\n",
    "nb_couches_cachees = 2  # Peut être 1, 2 ou 3\n",
    "\n",
    "# Paramètres de chaque couche\n",
    "nb_hidden1 = 128\n",
    "nb_hidden2 = 64\n",
    "nb_hidden3 = 32  # Utilisé uniquement si nb_couches_cachees = 3\n",
    "\n",
    "# Initialisation des poids\n",
    "w1 = torch.empty((nb_of_features, nb_hidden1), device=device, dtype=torch.float, requires_grad=True)\n",
    "torch.nn.init.normal_(w1, mean=0., std=.1)\n",
    "\n",
    "if nb_couches_cachees >= 2:\n",
    "    w_hidden2 = torch.empty((nb_hidden1, nb_hidden2), device=device, dtype=torch.float, requires_grad=True)\n",
    "    torch.nn.init.normal_(w_hidden2, mean=0., std=.1)\n",
    "\n",
    "if nb_couches_cachees == 3:\n",
    "    w_hidden3 = torch.empty((nb_hidden2, nb_hidden3), device=device, dtype=torch.float, requires_grad=True)\n",
    "    torch.nn.init.normal_(w_hidden3, mean=0., std=.1)\n",
    "    w2 = torch.empty((nb_hidden3, nb_of_ouputs), device=device, dtype=torch.float, requires_grad=True)\n",
    "else:\n",
    "    w2 = torch.empty((nb_hidden2 if nb_couches_cachees == 2 else nb_hidden1, nb_of_ouputs), device=device, dtype=torch.float, requires_grad=True)\n",
    "\n",
    "torch.nn.init.normal_(w2, mean=0., std=.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wqeJ9wNBm_84"
   },
   "outputs": [],
   "source": [
    "# # We create a 2 layer network (1 hidden, 1 output)\n",
    "# # choix utilisateur : nombre de neurones par couche\n",
    "\n",
    "\n",
    "# # choix utilisateur : nombre de couches\n",
    "# nb_hidden1 = 128  # Nombre de neurones de la première couche cachée\n",
    "# nb_hidden2 = 64   # Nombre de neurones de la deuxième couche cachée\n",
    "\n",
    "# # Poids pour la première couche cachée\n",
    "\n",
    "# w1 = torch.empty((nb_of_features, nb_hidden1), device=device, dtype=torch.float, requires_grad=True)\n",
    "# torch.nn.init.normal_(w1, mean=0., std=.1)\n",
    "# #torch.nn.init.normal_(w1, mean=1., std=.1)\n",
    "\n",
    "# # Poids pour la deuxième couche cachée\n",
    "\n",
    "# w_hidden2 = torch.empty((nb_hidden1, nb_hidden2), device=device, dtype=torch.float, requires_grad=True)\n",
    "# torch.nn.init.normal_(w_hidden2, mean=0., std=.1)\n",
    "\n",
    "# # Poids pour la couche de sortie\n",
    "\n",
    "# #w2 = torch.empty((nb_hidden1, nb_of_ouputs), device=device, dtype=torch.float, requires_grad=True)\n",
    "# w2 = torch.empty((nb_hidden2, nb_of_ouputs), device=device, dtype=torch.float, requires_grad=True)\n",
    "# torch.nn.init.normal_(w2, mean=0., std=.1)\n",
    "# #torch.nn.init.normal_(w2, mean=1., std=.1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Différentes fonctions d'activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpikeFunction_Classical_RELU(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)# Sauvegarde l'entrée pour l'utiliser dans la passe arrière.\n",
    "        return (input > 0).float()# Retourne 1 si l'entrée est positive, sinon 0.\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors# Récupère l'entrée sauvegardée depuis la passe avant.\n",
    "        grad_input = grad_output.clone()# Copie le gradient de sortie.\n",
    "        grad_input[input <= 0] = 0# Met à zéro le gradient pour les entrées non positives.\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "class SpikeFunction_Leaky_RELU(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)# Sauvegarde l'entrée pour la passe arrière.\n",
    "        return torch.where(input > 0, input, 0.01 * input)# Applique Leaky ReLU.\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input <= 0] *= 0.01# Multiplie par 0.01 pour les entrées négatives.\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "class SpikeFunction_Abs_RELU(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        return torch.abs(input)# Retourne la valeur absolue de l'entrée.\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()# Copie le gradient de sortie.\n",
    "        grad_input[input < 0] *= -1# Multiplie par -1 pour les entrées négatives.\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "class SpikeFunction_Sigmoid(Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input, alpha=1.0):\n",
    "        sigmoid = torch.sigmoid(alpha*input)# Calcule la sigmoïde de l'entrée.\n",
    "        ctx.save_for_backward(sigmoid, torch.tensor(alpha))# Sauvegarde la sortie pour la passe arrière.\n",
    "        return sigmoid\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        sigmoid, alpha = ctx.saved_tensors# Récupère la sortie sauvegardée (sigmoïde).\n",
    "        grad_input = grad_output * sigmoid * (1 - sigmoid)* alpha# Applique la dérivée de la sigmoïde.\n",
    "        return grad_input, None\n",
    "\n",
    "\n",
    "class SpikeFunction_Triangular(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, theta=0, delta=1):\n",
    "        ctx.save_for_backward(input)# Sauvegarde l'entrée pour la passe arrière.\n",
    "        ctx.theta = theta\n",
    "        ctx.delta = delta\n",
    "        \n",
    "        # Calcul de la fonction triangulaire en utilisant les paramètres theta et delta\n",
    "        # 1. input - theta : Décalage de l'entrée par rapport au paramètre theta\n",
    "        # 2. abs(input - theta) : Calcul de la valeur absolue de l'entrée par rapport au paramètre theta\n",
    "        # 3. 1 - abs(input - theta) / delta : Applique une forme triangulaire centrée autour de theta avec une largeur de delta\n",
    "        # 4. torch.maximum : Si la valeur est négative, on remplace par 0\n",
    "        grad_input = torch.maximum(1 - torch.abs(input - theta) / delta, torch.tensor(0.0, device=input.device))\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors  # Récupère l'entrée sauvegardée\n",
    "        grad_input = grad_output.clone()  # Copie le gradient de sortie\n",
    "        grad_input[torch.abs(input - ctx.theta) >= ctx.delta] = 0  # Pas de gradient si |input - theta| >= delta\n",
    "        grad_input[torch.abs(input - ctx.theta) < ctx.delta] = -1 / ctx.delta  # Gradient linéaire\n",
    "        return grad_input , None, None\n",
    "\n",
    "\n",
    "class SpikeFunction_Gaussian(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, alpha=1.0, theta=0.0):\n",
    "        \n",
    "        # Calcul de la fonction gaussienne avec les paramètres alpha et theta\n",
    "        #gaussian = torch.exp(-((input - theta) ** 2) / (2 * alpha ** 2))  # Gaussienne normalisee\n",
    "        #gaussian = torch.exp(-input ** 2)# Calcule la fonction gaussienne. de base\n",
    " \n",
    "        gaussian = torch.exp(-alpha * (input - theta) ** 2)\n",
    "\n",
    "        ctx.save_for_backward(input, gaussian)  # Sauvegarde de l'entrée et de la sortie pour la passe arrière\n",
    "        ctx.alpha = alpha\n",
    "        ctx.theta = theta\n",
    "        return gaussian\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, gaussian = ctx.saved_tensors# Récupère l'entrée et la sortie sauvegardées.\n",
    "        alpha = ctx.alpha\n",
    "        theta = ctx.theta\n",
    "        #grad_input = grad_output * (-2 * input * gaussian)# Applique la dérivée de la gaussienne. parametres de base\n",
    "        #grad_input = grad_output * ((input - theta) / (alpha ** 2)) * gaussian # Gaussienne normalisee\n",
    "        grad_input = grad_output * (-2 * alpha * (input - theta)) * gaussian\n",
    "\n",
    "        return grad_input,None,None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Cette class permet de calculer la sortie d'une fonction lors de la propagation avant et de personaliser la derivée lors de la retropropagation de l'erreur.\n",
    "Voir cet exemple pour plus de détails : https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html\n",
    "\"\"\"\n",
    "class SpikeFunction_Default(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Dans la passe avant, nous recevons un tenseur contenant l'entrée (potential-threshold).\n",
    "    Nous appliquons la fonction Heaviside et renvoyons un tenseur contenant la sortie.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        out = torch.zeros_like(input)\n",
    "        out[input > 0] = 1.0 # On génère une décharge quand (potential-threshold) > 0\n",
    "        return out\n",
    "\n",
    "    \"\"\"\n",
    "    Dans la passe arrière, nous recevons un tenseur contenant le gradient de l'erreur par rapport à la sortie.\n",
    "    Nous calculons le gradient de l'erreur par rapport à l'entrée en utilisant la dérivée de la fonction ReLu.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_relu = torch.ones_like(input) # La dérivée de la fonction ReLU\n",
    "        grad_relu[input < 0] = 0          # La dérivée de la fonction ReLU\n",
    "        return grad_output.clone()*grad_relu\n",
    "    \n",
    "\n",
    "# Énumération pour les fonctions d'activation\n",
    "class Fonctions_Activation(enum.Enum):\n",
    "    SpikeFunction_Classical_RELU = 1  # ReLU classique\n",
    "    SpikeFunction_Leaky_RELU = 2  # ReLU avec fuite\n",
    "    SpikeFunction_Abs_RELU = 3  # ReLU basé sur une fonction absolue\n",
    "    SpikeFunction_Default = 4  # Fonction par défaut (Heaviside)\n",
    "    SpikeFunction_Sigmoid = 5  # Fonction sigmoïde\n",
    "    SpikeFunction_Triangular = 6  # Fonction triangulaire\n",
    "    SpikeFunction_Gaussian = 7  # Fonction gaussienne\n",
    "    \n",
    "class CustomActivation(torch.nn.Module):\n",
    "    def __init__(self, activation_type, params=None):\n",
    "        \"\"\"\n",
    "        activation_type : Fonctions_Activation\n",
    "            Enumération représentant le type de fonction d'activation à utiliser.\n",
    "        \"\"\"\n",
    "        super(CustomActivation, self).__init__()\n",
    "        self.activation_type = activation_type\n",
    "        self.params = params or {}\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.activation_type == Fonctions_Activation.SpikeFunction_Classical_RELU:\n",
    "            return SpikeFunction_Classical_RELU.apply(x)\n",
    "        elif self.activation_type == Fonctions_Activation.SpikeFunction_Leaky_RELU:\n",
    "            return SpikeFunction_Leaky_RELU.apply(x)\n",
    "        elif self.activation_type == Fonctions_Activation.SpikeFunction_Abs_RELU:\n",
    "            return SpikeFunction_Abs_RELU.apply(x)\n",
    "        elif self.activation_type == Fonctions_Activation.SpikeFunction_Default:\n",
    "            return SpikeFunction_Default.apply(x)\n",
    "        elif self.activation_type == Fonctions_Activation.SpikeFunction_Sigmoid:\n",
    "            alpha = self.params.get(\"alpha\", 1.0)  # Valeur par défaut de alpha\n",
    "            return SpikeFunction_Sigmoid.apply(x, alpha)\n",
    "        elif self.activation_type == Fonctions_Activation.SpikeFunction_Triangular:\n",
    "            theta = self.params.get(\"theta\", 0) \n",
    "            delta = self.params.get(\"delta\", 1) \n",
    "            return SpikeFunction_Triangular.apply(x, theta, delta)\n",
    "        elif self.activation_type == Fonctions_Activation.SpikeFunction_Gaussian:\n",
    "            alpha = self.params.get(\"alpha\", 1.0)  # Valeur par défaut de alpha\n",
    "            theta = self.params.get(\"theta\", 0.0)  # Valeur par défaut de theta\n",
    "            return SpikeFunction_Gaussian.apply(x,alpha,theta)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation type.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"background-color: yellow; color: darkblue; display: inline-block; padding: 0.3em;\">Choix Utilisateur</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valeurs à modifier par l'utilisateur\n",
    "selected_function = 2  # Choix de la fonction d'activation \n",
    "#1 = ReLU classique, 2 = Leaky ReLU, 3 = Abs ReLU, 4 = Heaviside et dérivée ReLU classique, 5 = Sigmoïde, 6 = Triangulaire, 7 = Gaussienne\n",
    "params_sigmoid = {\"alpha\": 2.0} #Valeurs testées :  0.5, 1.0, 2\n",
    "params_triangular = {\"theta\": 0, \"delta\": 1}# Valeurs testées :(0, 1), (0, 2), (1, 1), (-1, 1)\n",
    "params_gaussian = {\"alpha\": 1.0, \"theta\": 0.0}# Valeurs testées :(1.0, 0.0), (3, 0.0), (1.0, 1.0), (1.0, -1.0)\n",
    "#\n",
    "\n",
    "\n",
    "# Conversion des choix en énumérations\n",
    "#selected_method_enum = Methodes_Apprentissage(selected_method)\n",
    "selected_function_enum = Fonctions_Activation(selected_function)\n",
    "\n",
    "Functions_parameters = None\n",
    "\n",
    "if selected_function_enum == Fonctions_Activation.SpikeFunction_Sigmoid:\n",
    "    Functions_parameters = params_sigmoid\n",
    "elif selected_function_enum == Fonctions_Activation.SpikeFunction_Triangular:\n",
    "    Functions_parameters = params_triangular\n",
    "elif selected_function_enum == Fonctions_Activation.SpikeFunction_Gaussian:\n",
    "    Functions_parameters = params_gaussian\n",
    "else:\n",
    "    Functions_parameters = {}  # Pas de paramètres spécifiques pour les autres fonctions\n",
    "\n",
    "\n",
    "# Création de la couche avec les paramètres spécifiques (si nécessaires)\n",
    "Layer = CustomActivation(activation_type=selected_function_enum, params=Functions_parameters)\n",
    "\n",
    "\n",
    "\n",
    "Function_name = selected_function_enum.name  # Utilisation de l'énumération\n",
    "# Affichage de la fonction et des paramètres sélectionnés\n",
    "print(f\"Nom de la fonction d'activation : {Function_name}\")\n",
    "print(f\"Paramètres associés : {Functions_parameters}\")\n",
    "\n",
    "if Function_name == 'SpikeFunction_Default':\n",
    "    print(\"LANCEMENT PAR DEFAUT\")\n",
    "\n",
    "\n",
    "# Exemple d'utilisation avec torch.Tensor\n",
    "#x = torch.randn(10)  # Exemple de tenseur d'entrée\n",
    "#custom_layer = CustomLayer(selected_method_enum, selected_function_enum)\n",
    "#output = custom_layer(x)\n",
    "#print(\"Sortie de la couche personnalisée : \", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implémentation dynamique LIF \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xAXZCMNsnIri"
   },
   "outputs": [],
   "source": [
    "def run_spiking_layer(input_spike_train, layer_weights, tau_v=20*units.ms, tau_i=5*units.ms, v_threshold=1.0):\n",
    "    \"\"\"Here we implement a current-LIF dynamic in PyTorch\"\"\"\n",
    "\n",
    "    # First, we multiply the input spike train by the weights of the current layer to get the current that will be added\n",
    "    # We can calculate this beforehand because the weights are constant in the forward pass (no plasticity)\n",
    "    input_current = torch.einsum(\"abc,bd->adc\", (input_spike_train, layer_weights))  # Equivalent to a matrix multiplication for tensors of dim > 2 using Einstein's Notation\n",
    "\n",
    "    recorded_spikes = []  # Array of the output spikes at each time t\n",
    "    membrane_potential_at_t = torch.zeros((input_spike_train.shape[0], layer_weights.shape[-1]), device=device, dtype=torch.float)\n",
    "    membrane_current_at_t = torch.zeros((input_spike_train.shape[0], layer_weights.shape[-1]), device=device, dtype=torch.float)\n",
    "\n",
    "    for t in range(absolute_duration):  # For every timestep\n",
    "        # Apply the leak\n",
    "        membrane_potential_at_t = float(np.exp(-dt/tau_v))*membrane_potential_at_t # Using tau_v with euler or exact method\n",
    "        membrane_current_at_t = float(np.exp(-dt/tau_i))*membrane_current_at_t # Using tau_i with euler or exact method\n",
    "\n",
    "        # Select the input current at time t\n",
    "        input_at_t = input_current[:, :, t]\n",
    "\n",
    "        # Integrate the input current\n",
    "        membrane_current_at_t += input_at_t\n",
    "\n",
    "        # Integrate the input to the membrane potential\n",
    "\n",
    "        membrane_potential_at_t += membrane_current_at_t *float(dt/tau_v) #! *float(dt/tau_v)??\n",
    "        \n",
    "\n",
    "        # Apply the non-differentiable function\n",
    "        if Function_name != 'SpikeFunction_Default':\n",
    "            recorded_spikes_at_t = Layer(membrane_potential_at_t - v_threshold)\n",
    "        else:\n",
    "            recorded_spikes_at_t = SpikeFunction_Default.apply(membrane_potential_at_t - v_threshold)\n",
    "     \n",
    "\n",
    "        \n",
    "        recorded_spikes.append(recorded_spikes_at_t)\n",
    "\n",
    "        # Reset the spiked neurons\n",
    "        membrane_potential_at_t[membrane_potential_at_t > v_threshold] = 0\n",
    "\n",
    "    recorded_spikes = torch.stack(recorded_spikes, dim=2) # Stack over time axis (Array -> Tensor)\n",
    "    return recorded_spikes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNEAhvjlnVqF"
   },
   "source": [
    "# 2) Entrainement et Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dPyehEEZzc4x"
   },
   "outputs": [],
   "source": [
    "print(f\"Fonction sélectionnée : {Function_name}\")\n",
    "if Functions_parameters is None:\n",
    "    print(\"Paramètres associés : Paramètres initiaux\")\n",
    "else:\n",
    "    print(\"Paramètres associés :\", \", \".join(f\"{key}: {value}\" for key, value in Functions_parameters.items()))\n",
    "print(f\"Nb couches cachés : {nb_couches_cachees}\")\n",
    "\n",
    "    \n",
    "# Set-up training\n",
    "correct_label_count = 0\n",
    "\n",
    "# Choix utilisateur : nombre d'époques et la taille des lots\n",
    "nb_of_epochs = 20\n",
    "batch_size = 256  # The backpropagation is done after every batch, but a batch here is also used for memory requirements\n",
    "number_of_batches = len(train_indices) // batch_size\n",
    "\n",
    "#params = [w1,w2]  # Trainable parameters\n",
    "#params = [w1, w_hidden2,w2]  # Trainable parameters\n",
    "# Liste des paramètres à entraîner\n",
    "params = [w1, w2]\n",
    "if nb_couches_cachees >= 2:\n",
    "    params.insert(1, w_hidden2)\n",
    "if nb_couches_cachees == 3:\n",
    "    params.insert(2, w_hidden3)\n",
    "\n",
    "# Choix utilisateur : taux d'apprentissage lr\n",
    "optimizer = torch.optim.Adam(params, lr=0.01, amsgrad=True)\n",
    "loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "# Initialisation des listes pour stocker les poids\n",
    "w1_history = []\n",
    "w_hidden2_history= []\n",
    "w2_history = []\n",
    "\n",
    "loss_history = {}\n",
    "loss_history[Function_name] = []\n",
    "\n",
    "\n",
    "for e in range(nb_of_epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch in np.array_split(train_indices, number_of_batches):\n",
    "        # Select batch and convert to tensors\n",
    "        batch_spike_train = torch.FloatTensor(spike_train[batch].todense()).to(device)\n",
    "        batch_labels = torch.LongTensor(y[batch, np.newaxis]).to(device)\n",
    "\n",
    "        # Here we create a target spike count (10 spikes for wrong label, 100 spikes for true label) in a one-hot fashion\n",
    "        # This approach is seen in Shrestha & Orchard (2018) https://arxiv.org/pdf/1810.08646.pdf\n",
    "        # Code available at https://github.com/bamsumit/slayerPytorch\n",
    "        min_spike_count = 10 * torch.ones((batch.shape[0], 10), device=device, dtype=torch.float)\n",
    "        target_output = min_spike_count.scatter_(1, batch_labels, 100.0)\n",
    "\n",
    "        # Forward propagation\n",
    "\n",
    "        # 1 couches cahée\n",
    "\n",
    "        #layer_1_spikes = run_spiking_layer(batch_spike_train, w1)\n",
    "        #layer_2_spikes = run_spiking_layer(layer_1_spikes, w2)\n",
    "        #network_output = torch.sum(layer_2_spikes, 2)  # Count the spikes over time axis\n",
    "\n",
    "        # 2 couches cahées\n",
    "\n",
    "        # Passe avant avec sommation temporelle\n",
    "        #layer_1_spikes = run_spiking_layer(batch_spike_train, w1)  # Première couche cachée\n",
    "        #layer_2_spikes = run_spiking_layer(layer_1_spikes, w_hidden2)  # Deuxième couche cachée\n",
    "        #output_spikes = run_spiking_layer(layer_2_spikes, w2)  # Couche de sortie\n",
    "        #network_output = torch.sum(output_spikes, 2)  # Sommation sur l'axe temporel\n",
    "\n",
    "        #loss = loss_fn(network_output, target_output)\n",
    "\n",
    "        # Forward propagation\n",
    "        layer_1_spikes = run_spiking_layer(batch_spike_train, w1)\n",
    "        if nb_couches_cachees == 1:\n",
    "            output_spikes = run_spiking_layer(layer_1_spikes, w2)\n",
    "        elif nb_couches_cachees == 2:\n",
    "            layer_2_spikes = run_spiking_layer(layer_1_spikes, w_hidden2)\n",
    "            output_spikes = run_spiking_layer(layer_2_spikes, w2)\n",
    "        else:  # nb_couches_cachees == 3\n",
    "            layer_2_spikes = run_spiking_layer(layer_1_spikes, w_hidden2)\n",
    "            layer_3_spikes = run_spiking_layer(layer_2_spikes, w_hidden3)\n",
    "            output_spikes = run_spiking_layer(layer_3_spikes, w2)\n",
    "\n",
    "        network_output = torch.sum(output_spikes, 2)  # Sommation temporelle\n",
    "        loss = loss_fn(network_output, target_output)\n",
    "\n",
    "\n",
    "        # Backward propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    #print(\"\\nW1: \",w1.grad)  # Pour vérifier les gradients de w1\n",
    "    #print(\"W2: \",w2.grad)  # Pour vérifier les gradients de w2   \n",
    "    #VALIDATION\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in np.array_split(validation_indices,  len(validation_indices) // batch_size):\n",
    "\n",
    "            validation_spike_train = torch.FloatTensor(spike_train[batch].todense()).to(device)\n",
    "\n",
    "            # Same forward propagation as before\n",
    "            # 1 couche cachée\n",
    "            #layer_1_spikes = run_spiking_layer(validation_spike_train, w1)\n",
    "            #layer_2_spikes = run_spiking_layer(layer_1_spikes, w2)\n",
    "            #network_output = torch.sum(layer_2_spikes, 2)  # Count the spikes over time axis\n",
    "\n",
    "            # 2 couches cachées\n",
    "            #layer_1_spikes = run_spiking_layer(validation_spike_train, w1)\n",
    "            #layer_2_spikes = run_spiking_layer(layer_1_spikes, w_hidden2)\n",
    "            #output_spikes = run_spiking_layer(layer_2_spikes, w2)\n",
    "            #network_output = torch.sum(output_spikes, 2)  # Sommation sur l'axe temporel\n",
    "            layer_1_spikes = run_spiking_layer(validation_spike_train, w1)\n",
    "            if nb_couches_cachees == 1:\n",
    "                output_spikes = run_spiking_layer(layer_1_spikes, w2)\n",
    "            elif nb_couches_cachees == 2:\n",
    "                layer_2_spikes = run_spiking_layer(layer_1_spikes, w_hidden2)\n",
    "                output_spikes = run_spiking_layer(layer_2_spikes, w2)\n",
    "            else:  # nb_couches_cachees == 3\n",
    "                layer_2_spikes = run_spiking_layer(layer_1_spikes, w_hidden2)\n",
    "                layer_3_spikes = run_spiking_layer(layer_2_spikes, w_hidden3)\n",
    "                output_spikes = run_spiking_layer(layer_3_spikes, w2)\n",
    "            network_output = torch.sum(output_spikes, 2)\n",
    "\n",
    "\n",
    "\n",
    "            #print(\"network_output shape:\", network_output.shape)  # Affiche la forme de la sortie\n",
    "            #print(\"y[batch] shape:\", y[batch].shape)  # Affiche la forme des labels\n",
    "\n",
    "\n",
    "            # Do the prediction by selecting the output neuron with the most number of spikes\n",
    "            _, am = torch.max(network_output, 1)\n",
    "            #print(f\"Prédictions : {am.cpu().numpy()}\")  # Affiche les prédictions #! QUE DES 0!!!!!!!!\n",
    "            #print(f\"Labels réels : {y[batch]}\")  # Affiche les labels réels\n",
    "            correct_label_count += np.sum(am.detach().cpu().numpy() == y[batch])\n",
    "\n",
    "        \n",
    "\n",
    "    print(\"\\nEpoch %i -- loss : %.4f\" %(e+1, epoch_loss / number_of_batches))\n",
    "    avg_epoch_loss = epoch_loss / number_of_batches\n",
    "    loss_history[Function_name].append(avg_epoch_loss)\n",
    "    \n",
    "    # Affichage des 1er, 2e et dernier poids de w1 et w2\n",
    "    print(f\"Weight evolution - w1: First = {w1.detach().cpu().numpy()[0][0]}, \"\n",
    "      f\"Second = {w1.detach().cpu().numpy()[1][0]}, \"\n",
    "      f\"Last = {w1.detach().cpu().numpy()[-1][0]}, \"\n",
    "      f\"Weight at (12, 25) = {w1.detach().cpu().numpy()[12][25]}\")\n",
    "\n",
    "    #print(f\"Weight evolution - w1: First = {w1.detach().cpu().numpy()[0][0]}, Second = {w1.detach().cpu().numpy()[1][0]}, Last = {w1.detach().cpu().numpy()[-1][0]}\")\n",
    "    if nb_couches_cachees >= 2:\n",
    "        print(f\"Weight evolution - w_hidden2: First = {w_hidden2.detach().cpu().numpy()[0][0]}, Second = {w_hidden2.detach().cpu().numpy()[1][0]}, Last = {w_hidden2.detach().cpu().numpy()[-1][0]}, Weight at (12, 25) = {w_hidden2.detach().cpu().numpy()[12][25]}\")\n",
    "        w_hidden2_history.append(w_hidden2.detach().cpu().numpy().copy())\n",
    "\n",
    "    print(f\"Weight evolution - w2: First = {w2.detach().cpu().numpy()[0][0]}, Second = {w2.detach().cpu().numpy()[1][0]}, Last = {w2.detach().cpu().numpy()[-1][0]}\")\n",
    "    w1_history.append(w1.detach().cpu().numpy().copy())  \n",
    "    w2_history.append(w2.detach().cpu().numpy().copy())\n",
    "\n",
    "print(\"VALIDATION : Model accuracy on test set: %.3f\" % (correct_label_count / len(validation_indices)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0j4qQrgVnihY"
   },
   "source": [
    "# 3) Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NgQyFnPetmxQ"
   },
   "outputs": [],
   "source": [
    "# Test the accuracy of the model\n",
    "correct_label_count = 0\n",
    "\n",
    "# We only need to batchify the test set for memory requirements\n",
    "for batch in np.array_split(test_indices,  len(test_indices) // batch_size):\n",
    "    test_spike_train = torch.FloatTensor(spike_train[batch].todense()).to(device)\n",
    "\n",
    "    # Same forward propagation as before\n",
    "    #layer_1_spikes = run_spiking_layer(test_spike_train, w1)\n",
    "    #layer_2_spikes = run_spiking_layer(layer_1_spikes, w2)\n",
    "    #network_output = torch.sum(layer_2_spikes, 2)  # Count the spikes over time axis\n",
    "    layer_1_spikes = run_spiking_layer(test_spike_train, w1)\n",
    "    if nb_couches_cachees == 1:\n",
    "        output_spikes = run_spiking_layer(layer_1_spikes, w2)\n",
    "    elif nb_couches_cachees == 2:\n",
    "        layer_2_spikes = run_spiking_layer(layer_1_spikes, w_hidden2)\n",
    "        output_spikes = run_spiking_layer(layer_2_spikes, w2)\n",
    "    else:  # nb_couches_cachees == 3\n",
    "        layer_2_spikes = run_spiking_layer(layer_1_spikes, w_hidden2)\n",
    "        layer_3_spikes = run_spiking_layer(layer_2_spikes, w_hidden3)\n",
    "        output_spikes = run_spiking_layer(layer_3_spikes, w2)\n",
    "\n",
    "    network_output = torch.sum(output_spikes, 2)  # Count the spikes over time axis\n",
    "\n",
    "    \n",
    "\n",
    "    # Do the prediction by selecting the output neuron with the most number of spikes\n",
    "    _, am = torch.max(network_output, 1)\n",
    "    correct_label_count += np.sum(am.detach().cpu().numpy() == y[batch])\n",
    "\n",
    "print(\"Model accuracy on test set: %.3f\" % (correct_label_count / len(test_indices)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Graphiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Évolution des poids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1_changes = []\n",
    "def analyze_weight_changes(weight_history, layer_name):\n",
    "    \"\"\"\n",
    "    Analyse les changements de poids dans une couche donnée.\n",
    "    \n",
    "    Arguments:\n",
    "    - weight_history : historique des poids (numpy array)\n",
    "    - layer_name : nom de la couche pour l'affichage des résultats\n",
    "    \n",
    "    Retourne :\n",
    "    - Nombre total de poids\n",
    "    - Nombre de poids ayant changé\n",
    "    - Pourcentage de poids modifiés\n",
    "    - Pourcentage de poids non modifiés\n",
    "    \"\"\"\n",
    "    weight_history = np.array(weight_history)\n",
    "    changes = []  # Liste des indices où les poids changent\n",
    "    \n",
    "    for i in range(weight_history.shape[1]):\n",
    "        for j in range(weight_history.shape[2]):\n",
    "            if weight_history[-1, i, j] != weight_history[0, i, j]:\n",
    "                changes.append((i, j))\n",
    "    if layer_name == \"w1\":\n",
    "        w1_changes = changes\n",
    "    # Résultats\n",
    "    total_weights = weight_history.shape[1] * weight_history.shape[2]\n",
    "    changed_weights = len(changes)\n",
    "    percentage_changed = (changed_weights / total_weights) * 100\n",
    "    percentage_not_changed = 100 - percentage_changed\n",
    "    \n",
    "    # Affichage\n",
    "    #if changes:\n",
    "        #print(f\"Indices des poids de {layer_name} ayant changé : {changes}\")\n",
    "    #else:\n",
    "        #print(f\"Les poids de {layer_name} n'ont pas changé.\")\n",
    "    \n",
    "    #print(f\"\\nNombre total de poids dans {layer_name} : {total_weights}\")\n",
    "    #print(f\"Nombre de poids ayant changé dans {layer_name} : {changed_weights}\")\n",
    "    #print(f\"Pourcentage de poids ayant changé dans {layer_name} : {percentage_changed:.2f}%\")\n",
    "    print(f\"Pourcentage de poids n'ayant pas changé dans {layer_name} : {percentage_not_changed:.2f}%\")\n",
    "    \n",
    "    return total_weights, changed_weights, percentage_changed, percentage_not_changed\n",
    "\n",
    "# Analyse des couches\n",
    "total_w1, changed_w1, _, _ = analyze_weight_changes(w1_history, \"w1\")\n",
    "total_hidden2, changed_hidden2, _, _ = analyze_weight_changes(w_hidden2_history, \"w_hidden2\")\n",
    "total_w2, changed_w2, _, _ = analyze_weight_changes(w2_history, \"w2\")\n",
    "\n",
    "# Calcul global\n",
    "total_weights_all = total_w1 + total_hidden2 + total_w2\n",
    "changed_weights_all = changed_w1 + changed_hidden2 + changed_w2\n",
    "percentage_changed_all = (changed_weights_all / total_weights_all) * 100\n",
    "percentage_not_changed_all = 100 - percentage_changed_all\n",
    "\n",
    "print(f\"\\nPourcentage de poids ayant changé au total : {percentage_changed_all:.2f}%\")\n",
    "print(f\"Pourcentage de poids n'ayant pas changé au total : {percentage_not_changed_all:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Fonction sélectionnée : {Function_name}\")\n",
    "\n",
    "if Functions_parameters is None:\n",
    "    params_description= \"avec aramètres initiaux\"\n",
    "else:\n",
    "    params_description = \" , \".join(f\"{key}: {value}\" for key, value in Functions_parameters.items())\n",
    "# Convertir w1_history et w2_history en tableaux numpy pour faciliter les manipulations\n",
    "print(f\"Fonction sélectionnée : {params_description}\")\n",
    "\n",
    "\n",
    "def plot_weight_evolution(weight_history, indices_to_plot, layer_name, Function_name, params_description):\n",
    "    \"\"\"\n",
    "    Trace l'évolution des poids pour une couche spécifique.\n",
    "    \n",
    "    Arguments :\n",
    "    - weight_history : historique des poids (numpy array)\n",
    "    - indices_to_plot : indices des poids à tracer\n",
    "    - layer_name : nom de la couche (ex : w1, w2, w_hidden2)\n",
    "    - Function_name : nom de la fonction utilisée\n",
    "    - params_description : description des paramètres de la fonction\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    \n",
    "    for i, idx in enumerate(indices_to_plot):\n",
    "        weight = weight_history[:, idx, 0]  # Poids pour le premier neurone cible\n",
    "        label = f\"Poids {layer_name}[{idx},0]\"\n",
    "        plt.plot(weight, label=label)\n",
    "    \n",
    "    plt.title(f\"{Function_name} {params_description}, Évolution des poids de {layer_name}\")\n",
    "    plt.xlabel(\"Époque\")\n",
    "    plt.ylabel(\"Valeur du poids\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Préparer les données\n",
    "w1_history = np.array(w1_history)\n",
    "w2_history = np.array(w2_history)\n",
    "w_hidden2_history = np.array(w_hidden2_history)  # Ajouter pour w_hidden2\n",
    "additional_indices = []\n",
    "additional_indices = w1_changes[:2]\n",
    "\n",
    "# Définir les indices des poids à tracer\n",
    "indices_w1 = [0, 1, 111, 222, 333, -1] + [idx[0] for idx in additional_indices]\n",
    "indices_w2 = [0, 1, 50, 60, 62, -1]\n",
    "indices_hidden2 = [0, 10, 20, 30, 40, -1]  # Ajouter des indices pour w_hidden2 si nécessaire\n",
    "\n",
    "# Afficher les graphiques\n",
    "print(f\"Fonction sélectionnée : {Function_name}\")\n",
    "print(f\"Paramètres : {params_description}\")\n",
    "\n",
    "# Tracer les poids pour chaque couche\n",
    "plot_weight_evolution(w1_history, indices_w1, \"w1\", Function_name, params_description)\n",
    "plot_weight_evolution(w2_history, indices_w2, \"w2\", Function_name, params_description)\n",
    "plot_weight_evolution(w_hidden2_history, indices_hidden2, \"w_hidden2\", Function_name, params_description)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Évolution du loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exemple de dictionnaire contenant les pertes pour chaque fonction\n",
    "\n",
    "\n",
    "# Tracé des courbes\n",
    "plt.figure(figsize=(6, 5))\n",
    "for function_name, losses in loss_history.items():\n",
    "    epochs = range(1, len(losses) + 1)  # Crée un range d'époques correspondant à chaque liste de pertes\n",
    "    plt.plot(epochs, losses, marker='o', label=function_name)\n",
    "    print(f\"{function_name} {params_description} : {', '.join([f'{loss:.3f}' for loss in losses])}\")\n",
    "\n",
    "\n",
    "# Personnalisation du graphique\n",
    "plt.title(f\"{Function_name} {params_description}, Évolution des pertes en fonction des époques\", fontsize=10)\n",
    "plt.xlabel(\"Époques\", fontsize=12)\n",
    "plt.ylabel(\"Pertes\", fontsize=12)\n",
    "plt.legend(title=\"Fonctions\", fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Affichage\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
