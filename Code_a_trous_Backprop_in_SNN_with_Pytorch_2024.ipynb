{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWTf7H60Z0i_"
   },
   "source": [
    "# Rétropropagation de l'erreur dans les réseaux de neurones à décharges\n",
    "### GEI723, Automne 2024\n",
    "Ce notebook présente comment l'algorithme de la descente du gradient peut être adapté pour la rétropropagation de l'erreur dans les réseaux de neurones à décharges avec des fonctions d'activation non dérivables.\n",
    "\n",
    "Ce notebook est utilisé dans le cadre du cours GEI723 (Neuro-Computationnel).\n",
    "\n",
    "- **Encodage de l'entrée** : Les entrées sont encodées sous forme de trains de spikes.\n",
    "- **Structure du réseau** : Réseau de neurones à décharges avec plusieurs couches, utilisant différentes fonctions d'activation\n",
    "- **Études menées** : \n",
    "  - Impact des fonctions d'activation et des dérivées sur la performance.\n",
    "  - Analyse des méta-paramètres (nombre de couches, taux d'apprentissage, taille des lots).\n",
    "  - Comparaison entre réseaux avec et sans apprentissage sur certaines couches.\n",
    "- **Objectif du code** : Optimisation et analyse de la rétropropagation de l'erreur des réseaux de neurones à décharges.\n",
    "\n",
    "**Auteurs :**\n",
    "Clémence Lamballe\n",
    "Behrouz Nik-Nejad-Kazem-Pour\n",
    "Jean-Sébastien Giroux\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook a été inspiré du notebook créé par Ismaël Balafrej, Ph.D. avec Jean Rouat, Ph.D., ing., professeur et adapté par Ahmad El Ferdaoussi, Ph.D. et Arnaud Yarga, étud. Ph.D, dont le copyright et les auteurs sont:\n",
    "\n",
    "Copyright (c) 2019-2024, Université de Sherbrooke, groupe de recherche NECOTIS. Tous droits réservés.  \n",
    "Auteurs: Ismael Balafrej, Jean Rouat, adapté par Ahmad El Ferdaoussi et Arnaud Yarga\n",
    "\n",
    "\n",
    "Ce travail a lui même été adapté et inspiré des articles suivants:\n",
    "1. Surrogate Gradient Learning in Spiking Neural Networks by Zenke & Ganguli (2018) https://arxiv.org/pdf/1901.09948.pdf\n",
    "2. SLAYER: Spike Layer Error Reassignment in Time (2018) https://arxiv.org/pdf/1810.08646.pdf\n",
    "3. Biologically inspired alternatives to backpropagation through time for learning in recurrent neural nets (2019) https://arxiv.org/pdf/1901.09049.pdf\n",
    "\n",
    "\n",
    "Dans cet exemple de code la gestion du potentiel et de l'intensité du neurone est placé dans la gestion des couches.\n",
    "Vous pouvez décider de le faire autrement, par exemple dans la phase de propagation avant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpKksf2Ik4ga"
   },
   "source": [
    "# Packages et imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "IJGOX5B8F7ic"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: quantities in c:\\users\\cllam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.16.1)\n",
      "Requirement already satisfied: sparse in c:\\users\\cllam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.15.4)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\cllam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from quantities) (2.0.2)\n",
      "Requirement already satisfied: scipy>=0.19 in c:\\users\\cllam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sparse) (1.14.1)\n",
      "Requirement already satisfied: numba>=0.49 in c:\\users\\cllam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sparse) (0.60.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in c:\\users\\cllam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from numba>=0.49->sparse) (0.43.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install quantities sparse\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, model_selection, utils\n",
    "import torch\n",
    "import quantities as units\n",
    "from sparse import COO\n",
    "import enum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Préparation de la configuration pour le réseau de neurones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BspwdQSOk8U-"
   },
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ngz0VkgVmJB6"
   },
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Use the GPU unless there is none available.\n",
    "# If you don't have a CUDA enabled GPU, I recommned using Google Colab,\n",
    "# available at https://colab.research.google.com. Create a new notebook\n",
    "# and then go to Runtime -> Change runtime type -> Hardware accelerator -> GPU\n",
    "# Colab gives you access to up to 12 free continuous hours of a fairly recent GPU.\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frOGRjz0mL_5"
   },
   "source": [
    "### Préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "hp-KevLKmLLP"
   },
   "outputs": [],
   "source": [
    "# Let's download the MNIST dataset, available at https://www.openml.org/d/554\n",
    "# You can edit the argument data_home to the directory of your choice.\n",
    "# The dataset will be downloaded there; the default directory is ~/scikit_learn_data/\n",
    "X, y = datasets.fetch_openml('mnist_784', version=1, return_X_y=True, data_home=None, as_frame=False)\n",
    "nb_of_samples, nb_of_features = X.shape\n",
    "# X = 70k samples, 28*28 features; y = 70k samples, 1 label (string)\n",
    "\n",
    "# Shuffle the dataset\n",
    "X, y = utils.shuffle(X, y)\n",
    "\n",
    "# Convert the labels (string) to integers for convenience\n",
    "y = np.array(y, dtype=int)\n",
    "nb_of_ouputs = np.max(y) + 1\n",
    "\n",
    "# We'll normalize our input data in the range [0, 1[.\n",
    "X = X / pow(2, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dgamMat1mXvu"
   },
   "source": [
    "### Conversion en décharges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "moU3ZUh8mSFG",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# And convert the data to a spike train using TTFS encoding\n",
    "dt = 1*units.ms\n",
    "duration_per_image = 100*units.ms\n",
    "absolute_duration = int(duration_per_image / dt)\n",
    "\n",
    "time_of_spike = (1 - X) * absolute_duration  # The brighter the pixel, the earlier the spike\n",
    "time_of_spike[X < .25] = 0  # \"Remove\" the spikes associated with darker pixels, which presumably carry less information\n",
    "\n",
    "sample_id, neuron_idx = np.nonzero(time_of_spike)\n",
    "\n",
    "# We use a sparse COO array to store the spikes for memory requirements\n",
    "# You can use the spike_train variable as if it were a tensor of shape (nb_of_samples, nb_of_features, absolute_duration)\n",
    "spike_train = COO((sample_id, neuron_idx, time_of_spike[sample_id, neuron_idx].astype(int)),\n",
    "                  np.ones_like(sample_id), shape=(nb_of_samples, nb_of_features, absolute_duration))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAxIwqtumyda"
   },
   "source": [
    "### Split entrainement/test/Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_of_samples = 70000\n",
      "\n",
      "train_indices = [    0     1     2 ... 55997 55998 55999]\n",
      "train_indices.shape = (56000,)\n",
      "\n",
      "validation_indices = [56000 56001 56002 ... 62997 62998 62999]\n",
      "validation_indices.shape = (7000,)\n",
      "\n",
      "test_indices = [63000 63001 63002 ... 69997 69998 69999]\n",
      "test_indices.shape = (7000,)\n",
      "\n",
      "total = 70000\n"
     ]
    }
   ],
   "source": [
    "# Nombre total d'échantillons\n",
    "print(\"nb_of_samples =\", nb_of_samples)\n",
    "\n",
    "# 80% pour l'entraînement\n",
    "nb_of_train_samples = int(nb_of_samples * 0.80)\n",
    "\n",
    "# 10% pour test\n",
    "nb_of_test_samples = int(nb_of_samples * 0.10)\n",
    "\n",
    "# 10% pour validation\n",
    "nb_of_validation_samples = nb_of_samples - nb_of_train_samples - nb_of_test_samples\n",
    "\n",
    "# Création des indices\n",
    "train_indices = np.arange(nb_of_train_samples)\n",
    "validation_indices = np.arange(nb_of_train_samples, nb_of_train_samples + nb_of_validation_samples)\n",
    "test_indices = np.arange(nb_of_train_samples + nb_of_validation_samples, nb_of_samples)\n",
    "\n",
    "\n",
    "print(\"\\ntrain_indices =\", train_indices)\n",
    "print(\"train_indices.shape =\", train_indices.shape)\n",
    "\n",
    "\n",
    "print(\"\\nvalidation_indices =\", validation_indices)\n",
    "print(\"validation_indices.shape =\", validation_indices.shape)\n",
    "\n",
    "print(\"\\ntest_indices =\", test_indices)\n",
    "print(\"test_indices.shape =\", test_indices.shape)\n",
    "\n",
    "\n",
    "total_samples = train_indices.shape[0] + test_indices.shape[0] + validation_indices.shape[0]\n",
    "print(\"\\ntotal =\", total_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZWlfjmhjmdPz"
   },
   "source": [
    "### Création du réseau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "wqeJ9wNBm_84"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0166, -0.0201, -0.0915,  ...,  0.1204, -0.0279,  0.1034],\n",
       "        [ 0.0904, -0.0630, -0.0854,  ...,  0.1121, -0.0862, -0.1139],\n",
       "        [ 0.0663, -0.0444,  0.0240,  ...,  0.0991, -0.0835,  0.0579],\n",
       "        ...,\n",
       "        [-0.2037,  0.0690,  0.0162,  ..., -0.0634,  0.0101, -0.0868],\n",
       "        [ 0.0544, -0.1185,  0.1433,  ..., -0.0279, -0.0658,  0.1325],\n",
       "        [ 0.0045,  0.0288,  0.0155,  ..., -0.0239, -0.0366, -0.0218]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We create a 2 layer network (1 hidden, 1 output)\n",
    "nb_hidden = 128  # Number of hidden neurons\n",
    "\n",
    "w1 = torch.empty((nb_of_features, nb_hidden), device=device, dtype=torch.float, requires_grad=True)\n",
    "torch.nn.init.normal_(w1, mean=0., std=.1)\n",
    "\n",
    "w2 = torch.empty((nb_hidden, nb_of_ouputs), device=device, dtype=torch.float, requires_grad=True)\n",
    "torch.nn.init.normal_(w2, mean=0., std=.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classe pour la rétropropagation de l'erreur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "rrZ3qeWfnBwj"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Cette class permet de calculer la sortie d'une fonction lors de la propagation avant et de personaliser la derivée lors de la retropropagation de l'erreur.\n",
    "Voir cet exemple pour plus de détails : https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html\n",
    "\"\"\"\n",
    "class SpikeFunction_Default(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    Dans la passe avant, nous recevons un tenseur contenant l'entrée (potential-threshold).\n",
    "    Nous appliquons la fonction Heaviside et renvoyons un tenseur contenant la sortie.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        out = torch.zeros_like(input)\n",
    "        out[input > 0] = 1.0 # On génère une décharge quand (potential-threshold) > 0\n",
    "        return out\n",
    "\n",
    "    \"\"\"\n",
    "    Dans la passe arrière, nous recevons un tenseur contenant le gradient de l'erreur par rapport à la sortie.\n",
    "    Nous calculons le gradient de l'erreur par rapport à l'entrée en utilisant la dérivée de la fonction ReLu.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_relu = torch.ones_like(input) # La dérivée de la fonction ReLU\n",
    "        grad_relu[input < 0] = 0          # La dérivée de la fonction ReLU\n",
    "        return grad_output.clone()*grad_relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SpikeFunction_Classical_RELU(torch.autograd.Function):\n",
    "#     \"\"\"\n",
    "#     Variante classique de ReLU pour la propagation avant et arrière.\n",
    "#     \"\"\"\n",
    "#     @staticmethod\n",
    "#     def forward(ctx, input):\n",
    "#         ctx.save_for_backward(input)\n",
    "#         out = torch.zeros_like(input)\n",
    "#         out[input > 0] = 1.0 # On génère une décharge quand (potential-threshold) > 0\n",
    "#         return out\n",
    "\n",
    "#     @staticmethod\n",
    "#     def backward(ctx, grad_output):\n",
    "#         \"\"\"\n",
    "#         Passe arrière : dérivée de ReLU classique.\n",
    "#         \"\"\"\n",
    "#         input, = ctx.saved_tensors\n",
    "#         grad_relu = torch.ones_like(input)\n",
    "#         grad_relu[input <= 0] = 0  # Dérivée nulle pour les valeurs négatives\n",
    "#         return grad_output.clone() * grad_relu\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# class SpikeFunction_Leaky_RELU(torch.autograd.Function):\n",
    "#     \"\"\"\n",
    "#     Variante utilisant Leaky ReLU pour la propagation avant et arrière.\n",
    "#     \"\"\"\n",
    "#     @staticmethod # Smooth change aussi ici\n",
    "#     def forward(ctx, input):\n",
    "#         ctx.save_for_backward(input)\n",
    "#         out = torch.zeros_like(input)\n",
    "#         out[input > 0] = 1.0 # On génère une décharge quand (potential-threshold) > 0\n",
    "#         return out\n",
    "\n",
    "#     @staticmethod# Surregate invente ici\n",
    "#     def backward(ctx, grad_output):\n",
    "#         \"\"\"\n",
    "#         Passe arrière : applique la dérivée de Leaky ReLU.\n",
    "#         \"\"\"\n",
    "#         input, = ctx.saved_tensors\n",
    "#         alpha = 0.01 #!ctx.alpha\n",
    "#         grad_leaky_relu = torch.ones_like(input)\n",
    "#         grad_leaky_relu[input < 0] = alpha  # La pente alpha pour les valeurs négatives\n",
    "#         return grad_output.clone() * grad_leaky_relu, None\n",
    "\n",
    "\n",
    "# class SpikeFunction_Abs_RELU(torch.autograd.Function):\n",
    "#     \"\"\"\n",
    "#     Variante utilisant Abs ReLU pour la propagation avant et arrière.\n",
    "#     \"\"\"\n",
    "#     @staticmethod\n",
    "#     def forward(ctx, input):\n",
    "#         ctx.save_for_backward(input)\n",
    "#         out = torch.zeros_like(input)\n",
    "#         out[input > 0] = 1.0 # On génère une décharge quand (potential-threshold) > 0\n",
    "#         return out\n",
    "\n",
    "#     @staticmethod\n",
    "#     def backward(ctx, grad_output):\n",
    "#         \"\"\"\n",
    "#         Passe arrière : dérivée constante 1 pour Abs ReLU.\n",
    "#         \"\"\"\n",
    "#         input, = ctx.saved_tensors\n",
    "#         grad_abs_relu = torch.ones_like(input)  # La dérivée est constante 1\n",
    "#         return grad_output.clone() * grad_abs_relu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choix Utilisateur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Methodes_Apprentissage(enum.Enum):\n",
    "    Smooth =1 # Utilise une fonction d'activation lisse, calcul de la dérivée automatique\n",
    "    Surrogate = 2# Approximation pour calculer les gradients d'une fonction non lisse\n",
    "    Smooth_Surrogate =3# Combine Smooth et Surrogate pour différentes couches\n",
    "\n",
    "\n",
    "class Fonctions_Activation(enum.Enum):\n",
    "    SpikeFunction_Classical_RELU =1\n",
    "    SpikeFunction_Leaky_RELU=2\n",
    "    SpikeFunction_Abs_RELU=3\n",
    "    SpikeFunction_Default=4\n",
    "\n",
    "\n",
    "#Fonctions_Activation.SpikeFunction_Default\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valeurs à modifier par l'utilisateur\n",
    "selected_method = 1  # À modifier selon le choix de l'utilisateur\n",
    "selected_function = 5  # Choix de la fonction d'activation (5 = Sigmoid)\n",
    "\n",
    "\n",
    "# Énumération pour les méthodes d'apprentissage\n",
    "class Methodes_Apprentissage(enum.Enum):\n",
    "    Smooth = 1  # Utilise une fonction d'activation lisse, calcul de la dérivée automatique\n",
    "    Surrogate = 2  # Approximation pour calculer les gradients d'une fonction non lisse\n",
    "    Smooth_Surrogate = 3  # Combine Smooth et Surrogate pour différentes couches\n",
    "\n",
    "# Énumération pour les fonctions d'activation\n",
    "class Fonctions_Activation(enum.Enum):\n",
    "    SpikeFunction_Classical_RELU = 1  # ReLU classique\n",
    "    SpikeFunction_Leaky_RELU = 2  # ReLU avec fuite\n",
    "    SpikeFunction_Abs_RELU = 3  # ReLU basé sur une fonction absolue\n",
    "    SpikeFunction_Default = 4  # Fonction par défaut (Heaviside)\n",
    "    SpikeFunction_Sigmoid = 5  # Fonction sigmoïde\n",
    "    SpikeFunction_Triangular = 6  # Fonction triangulaire\n",
    "    SpikeFunction_Gaussian = 7  # Fonction gaussienne\n",
    "\n",
    "# Fonction d'activation selon le choix\n",
    "def spike_function(x, fonction_activation, method_apprentissage):\n",
    "    if fonction_activation == Fonctions_Activation.SpikeFunction_Default:\n",
    "        return SpikeFunction_Default.apply(x)  # Utilisation de la fonction par défaut\n",
    "    elif fonction_activation == Fonctions_Activation.SpikeFunction_Classical_RELU:\n",
    "        if method_apprentissage == Methodes_Apprentissage.Surrogate:\n",
    "            # Derivée manuelle de ReLU classique (Surrogate)\n",
    "            return torch.relu(x)\n",
    "        else:\n",
    "            raise ValueError(\"ReLU classique nécessite la méthode Surrogate\")\n",
    "    elif fonction_activation == Fonctions_Activation.SpikeFunction_Leaky_RELU:\n",
    "        if method_apprentissage == Methodes_Apprentissage.Surrogate:\n",
    "            # Derivée manuelle de Leaky ReLU\n",
    "            alpha = 0.01\n",
    "            return torch.nn.functional.leaky_relu(x, negative_slope=alpha)\n",
    "        else:\n",
    "            raise ValueError(\"Leaky ReLU nécessite la méthode Surrogate\")\n",
    "    elif fonction_activation == Fonctions_Activation.SpikeFunction_Abs_RELU:\n",
    "        if method_apprentissage == Methodes_Apprentissage.Surrogate:\n",
    "            # Derivée manuelle de Abs ReLU\n",
    "            return torch.abs(torch.relu(x))\n",
    "        else:\n",
    "            raise ValueError(\"Abs ReLU nécessite la méthode Surrogate\")\n",
    "    elif fonction_activation == Fonctions_Activation.SpikeFunction_Sigmoid:\n",
    "        if method_apprentissage == Methodes_Apprentissage.Smooth:\n",
    "            # Fonction sigmoïde (différentiable)\n",
    "            return torch.sigmoid(x)\n",
    "        else:\n",
    "            raise ValueError(\"Sigmoid nécessite la méthode Smooth\")\n",
    "    elif fonction_activation == Fonctions_Activation.SpikeFunction_Triangular:\n",
    "        if method_apprentissage == Methodes_Apprentissage.Smooth:\n",
    "            # Fonction triangulaire approximée\n",
    "            return torch.maximum(torch.zeros_like(x), torch.minimum(x, torch.ones_like(x)))\n",
    "        else:\n",
    "            raise ValueError(\"Triangular nécessite la méthode Smooth\")\n",
    "    elif fonction_activation == Fonctions_Activation.SpikeFunction_Gaussian:\n",
    "        if method_apprentissage == Methodes_Apprentissage.Smooth:\n",
    "            # Fonction gaussienne approximée\n",
    "            return torch.exp(-x**2)\n",
    "        else:\n",
    "            raise ValueError(\"Gaussian nécessite la méthode Smooth\")\n",
    "    else:\n",
    "        raise ValueError(\"Fonction d'activation inconnue\")\n",
    "\n",
    "# Définition de la fonction par défaut (Heaviside)\n",
    "class SpikeFunction_Default(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        out = torch.zeros_like(input)\n",
    "        out[input > 0] = 1.0  # Applique la fonction Heaviside\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = torch.zeros_like(input)\n",
    "        grad_input[input > 0] = grad_output[input > 0]  # La dérivée de Heaviside est 0 sauf si input > 0\n",
    "        return grad_input\n",
    "\n",
    "# Classe de couche personnalisée\n",
    "class CustomLayer(torch.nn.Module):\n",
    "    def __init__(self, method_apprentissage, fonction_activation):\n",
    "        super(CustomLayer, self).__init__()\n",
    "        self.method_apprentissage = method_apprentissage\n",
    "        self.fonction_activation = fonction_activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.method_apprentissage == Methodes_Apprentissage.Smooth:\n",
    "            # Forward pour Smooth : fonctions activations différentiables\n",
    "            x = spike_function(x, self.fonction_activation, self.method_apprentissage)  # Activation lisse\n",
    "        elif self.method_apprentissage == Methodes_Apprentissage.Surrogate:\n",
    "            # Forward pour Surrogate : approximation de la dérivée\n",
    "            x = spike_function(x, self.fonction_activation, self.method_apprentissage)  # Activation avec Surrogate\n",
    "        elif self.method_apprentissage == Methodes_Apprentissage.Smooth_Surrogate:\n",
    "            # Forward pour Smooth_Surrogate : combinaison de Smooth et Surrogate\n",
    "            x = spike_function(x, self.fonction_activation, self.method_apprentissage) * torch.sigmoid(x)  # Combinaison\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        # La logique du backward est définie dans chaque fonction d'activation via la méthode apply()\n",
    "        return grad_output\n",
    "\n",
    "\n",
    "# Conversion des choix en énumérations\n",
    "selected_method_enum = Methodes_Apprentissage(selected_method)\n",
    "selected_function_enum = Fonctions_Activation(selected_function)\n",
    "\n",
    "# Affichage des choix\n",
    "print(f\"Méthode d'apprentissage sélectionnée : {selected_method_enum.name}\")\n",
    "print(f\"Fonction d'activation sélectionnée : {selected_function_enum.name}\")\n",
    "\n",
    "# Exemple d'utilisation avec torch.Tensor\n",
    "#x = torch.randn(10)  # Exemple de tenseur d'entrée\n",
    "#custom_layer = CustomLayer(selected_method_enum, selected_function_enum)\n",
    "#output = custom_layer(x)\n",
    "#print(\"Sortie de la couche personnalisée : \", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fonction sélectionnée : SpikeFunction_Default\n"
     ]
    }
   ],
   "source": [
    "#! Choix utilisateur a change pour AU DESSUS\n",
    "#! A MODIF Choix utilisateur a change pour AU DESSUS\n",
    "\n",
    "Function_no = 4  # 1 = Relu, 2 = Leaky Relu, 3 = Abs Relu, 4 = Default Function initiale du code a trous\n",
    "# Mapping des fonctions d'activation disponibles\n",
    "activation_functions = {\n",
    "    #1: SpikeFunction_Classical_RELU,\n",
    "    #2: SpikeFunction_Leaky_RELU,\n",
    "    #3: SpikeFunction_Abs_RELU,\n",
    "    4:SpikeFunction_Default\n",
    "}\n",
    "\n",
    "# Récupération de la classe en fonction du choix de l'utilisateur\n",
    "SpikeFunction = activation_functions.get(Function_no, SpikeFunction_Default)\n",
    "\n",
    "# Imprimez le nom de la fonction sélectionnée\n",
    "Function_name = SpikeFunction.__name__\n",
    "print(f\"Fonction sélectionnée : {Function_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implémentation d'une couche de réseau de neurones à impulsions avec dynamique LIF basée sur le courant en PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "xAXZCMNsnIri"
   },
   "outputs": [],
   "source": [
    "def run_spiking_layer(input_spike_train, layer_weights, tau_v=20*units.ms, tau_i=5*units.ms, v_threshold=1.0):\n",
    "    \"\"\"Here we implement a current-LIF dynamic in PyTorch\"\"\"\n",
    "\n",
    "    # First, we multiply the input spike train by the weights of the current layer to get the current that will be added\n",
    "    # We can calculate this beforehand because the weights are constant in the forward pass (no plasticity)\n",
    "    input_current = torch.einsum(\"abc,bd->adc\", (input_spike_train, layer_weights))  # Equivalent to a matrix multiplication for tensors of dim > 2 using Einstein's Notation\n",
    "\n",
    "    recorded_spikes = []  # Array of the output spikes at each time t\n",
    "    membrane_potential_at_t = torch.zeros((input_spike_train.shape[0], layer_weights.shape[-1]), device=device, dtype=torch.float)\n",
    "    membrane_current_at_t = torch.zeros((input_spike_train.shape[0], layer_weights.shape[-1]), device=device, dtype=torch.float)\n",
    "\n",
    "    for t in range(absolute_duration):  # For every timestep\n",
    "        # Apply the leak\n",
    "        membrane_potential_at_t = float(np.exp(-dt/tau_v))*membrane_potential_at_t # Using tau_v with euler or exact method\n",
    "        membrane_current_at_t = float(np.exp(-dt/tau_i))*membrane_current_at_t # Using tau_i with euler or exact method\n",
    "\n",
    "        # Select the input current at time t\n",
    "        input_at_t = input_current[:, :, t]\n",
    "\n",
    "        # Integrate the input current\n",
    "        membrane_current_at_t += input_at_t\n",
    "\n",
    "        # Integrate the input to the membrane potential\n",
    "\n",
    "        membrane_potential_at_t += membrane_current_at_t #*float(dt/tau_v) #! *float(dt/tau_v)??\n",
    "        #! si on rajoute le *, avec SpikeFunction_Default, on arrive pas a descendre le loss alors que sans oui\n",
    "\n",
    "        # Apply the non-differentiable function\n",
    "        #recorded_spikes_at_t = SpikeFunction.apply(membrane_potential_at_t - v_threshold)\n",
    "        recorded_spikes_at_t = SpikeFunction.apply(membrane_potential_at_t - v_threshold)\n",
    "        #, activation_type='relu'\n",
    "        #activation_type='leaky_relu', alpha=0.01\n",
    "        #activation_type='abs_relu'\n",
    "\n",
    "        \n",
    "        recorded_spikes.append(recorded_spikes_at_t)\n",
    "\n",
    "        # Reset the spiked neurons\n",
    "        membrane_potential_at_t[membrane_potential_at_t > v_threshold] = 0\n",
    "\n",
    "    recorded_spikes = torch.stack(recorded_spikes, dim=2) # Stack over time axis (Array -> Tensor)\n",
    "    return recorded_spikes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNEAhvjlnVqF"
   },
   "source": [
    "# 2) Entrainement et Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "dPyehEEZzc4x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fonction sélectionnée : SpikeFunction_Default\n",
      "\n",
      "Epoch 1 -- loss : 329.8574\n",
      "Weight evolution - w1: First = -0.11258398741483688, Second = -0.08833757787942886, Last = -0.02453634701669216, Weight at (12, 25) = -0.08552055805921555\n",
      "Weight evolution - w2: First = 0.3328050673007965, Second = 0.6578252911567688, Last = -0.36469849944114685\n",
      "\n",
      "Epoch 2 -- loss : 292.1398\n",
      "Weight evolution - w1: First = -0.11258398741483688, Second = -0.08833757787942886, Last = -0.02453634701669216, Weight at (12, 25) = -0.08552055805921555\n",
      "Weight evolution - w2: First = 0.5268695950508118, Second = 1.106713891029358, Last = -0.32680296897888184\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 44\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Backward propagation\u001b[39;00m\n\u001b[0;32m     43\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 44\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     48\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\cllam\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cllam\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cllam\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[1;32mc:\\Users\\cllam\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\function.py:307\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    302\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplementing both \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackward\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvjp\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for a custom \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    303\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction is not allowed. You should only implement one \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    304\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof them.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    305\u001b[0m     )\n\u001b[0;32m    306\u001b[0m user_fn \u001b[38;5;241m=\u001b[39m vjp_fn \u001b[38;5;28;01mif\u001b[39;00m vjp_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Function\u001b[38;5;241m.\u001b[39mvjp \u001b[38;5;28;01melse\u001b[39;00m backward_fn\n\u001b[1;32m--> 307\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43muser_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 26\u001b[0m, in \u001b[0;36mSpikeFunction_Default.backward\u001b[1;34m(ctx, grad_output)\u001b[0m\n\u001b[0;32m     24\u001b[0m grad_relu \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones_like(\u001b[38;5;28minput\u001b[39m) \u001b[38;5;66;03m# La dérivée de la fonction ReLU\u001b[39;00m\n\u001b[0;32m     25\u001b[0m grad_relu[\u001b[38;5;28minput\u001b[39m \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m          \u001b[38;5;66;03m# La dérivée de la fonction ReLU\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgrad_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m*\u001b[39mgrad_relu\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(f\"Fonction sélectionnée : {Function_name}\")\n",
    "\n",
    "# Set-up training\n",
    "correct_label_count = 0\n",
    "\n",
    "\n",
    "nb_of_epochs = 20\n",
    "batch_size = 256  # The backpropagation is done after every batch, but a batch here is also used for memory requirements\n",
    "number_of_batches = len(train_indices) // batch_size\n",
    "\n",
    "params = [w1, w2]  # Trainable parameters\n",
    "optimizer = torch.optim.Adam(params, lr=0.01, amsgrad=True)\n",
    "loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "\n",
    "# Initialisation des listes pour stocker les poids\n",
    "w1_history = []\n",
    "w2_history = []\n",
    "\n",
    "loss_history = {}\n",
    "loss_history[Function_name] = []\n",
    "\n",
    "\n",
    "for e in range(nb_of_epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch in np.array_split(train_indices, number_of_batches):\n",
    "        # Select batch and convert to tensors\n",
    "        batch_spike_train = torch.FloatTensor(spike_train[batch].todense()).to(device)\n",
    "        batch_labels = torch.LongTensor(y[batch, np.newaxis]).to(device)\n",
    "\n",
    "        # Here we create a target spike count (10 spikes for wrong label, 100 spikes for true label) in a one-hot fashion\n",
    "        # This approach is seen in Shrestha & Orchard (2018) https://arxiv.org/pdf/1810.08646.pdf\n",
    "        # Code available at https://github.com/bamsumit/slayerPytorch\n",
    "        min_spike_count = 10 * torch.ones((batch.shape[0], 10), device=device, dtype=torch.float)\n",
    "        target_output = min_spike_count.scatter_(1, batch_labels, 100.0)\n",
    "\n",
    "        # Forward propagation\n",
    "        layer_1_spikes = run_spiking_layer(batch_spike_train, w1)\n",
    "        layer_2_spikes = run_spiking_layer(layer_1_spikes, w2)\n",
    "        network_output = torch.sum(layer_2_spikes, 2)  # Count the spikes over time axis\n",
    "        loss = loss_fn(network_output, target_output)\n",
    "\n",
    "        # Backward propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    #print(\"\\nW1: \",w1.grad)  # Pour vérifier les gradients de w1\n",
    "    #print(\"W2: \",w2.grad)  # Pour vérifier les gradients de w2   \n",
    "    #VALIDATION\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in np.array_split(validation_indices,  len(validation_indices) // batch_size):\n",
    "\n",
    "            validation_spike_train = torch.FloatTensor(spike_train[batch].todense()).to(device)\n",
    "\n",
    "            # Same forward propagation as before\n",
    "            layer_1_spikes = run_spiking_layer(validation_spike_train, w1)\n",
    "            layer_2_spikes = run_spiking_layer(layer_1_spikes, w2)\n",
    "            network_output = torch.sum(layer_2_spikes, 2)  # Count the spikes over time axis\n",
    "\n",
    "            #print(\"network_output shape:\", network_output.shape)  # Affiche la forme de la sortie\n",
    "            #print(\"y[batch] shape:\", y[batch].shape)  # Affiche la forme des labels\n",
    "\n",
    "\n",
    "            # Do the prediction by selecting the output neuron with the most number of spikes\n",
    "            _, am = torch.max(network_output, 1)\n",
    "            #print(f\"Prédictions : {am.cpu().numpy()}\")  # Affiche les prédictions #! QUE DES 0!!!!!!!!\n",
    "            #print(f\"Labels réels : {y[batch]}\")  # Affiche les labels réels\n",
    "            correct_label_count += np.sum(am.detach().cpu().numpy() == y[batch])\n",
    "\n",
    "        \n",
    "\n",
    "    print(\"\\nEpoch %i -- loss : %.4f\" %(e+1, epoch_loss / number_of_batches))\n",
    "    avg_epoch_loss = epoch_loss / number_of_batches\n",
    "    loss_history[Function_name].append(avg_epoch_loss)\n",
    "    \n",
    "    # Affichage des 1er, 2e et dernier poids de w1 et w2\n",
    "    print(f\"Weight evolution - w1: First = {w1.detach().cpu().numpy()[0][0]}, \"\n",
    "      f\"Second = {w1.detach().cpu().numpy()[1][0]}, \"\n",
    "      f\"Last = {w1.detach().cpu().numpy()[-1][0]}, \"\n",
    "      f\"Weight at (12, 25) = {w1.detach().cpu().numpy()[12][25]}\")\n",
    "\n",
    "    #print(f\"Weight evolution - w1: First = {w1.detach().cpu().numpy()[0][0]}, Second = {w1.detach().cpu().numpy()[1][0]}, Last = {w1.detach().cpu().numpy()[-1][0]}\")\n",
    "    print(f\"Weight evolution - w2: First = {w2.detach().cpu().numpy()[0][0]}, Second = {w2.detach().cpu().numpy()[1][0]}, Last = {w2.detach().cpu().numpy()[-1][0]}\")\n",
    "    w1_history.append(w1.detach().cpu().numpy().copy())  \n",
    "    w2_history.append(w2.detach().cpu().numpy().copy())\n",
    "\n",
    "print(\"VALIDATION : Model accuracy on test set: %.3f\" % (correct_label_count / len(validation_indices)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0j4qQrgVnihY"
   },
   "source": [
    "# 3) Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NgQyFnPetmxQ"
   },
   "outputs": [],
   "source": [
    "# Test the accuracy of the model\n",
    "correct_label_count = 0\n",
    "\n",
    "# We only need to batchify the test set for memory requirements\n",
    "for batch in np.array_split(test_indices,  len(test_indices) // batch_size):\n",
    "    test_spike_train = torch.FloatTensor(spike_train[batch].todense()).to(device)\n",
    "\n",
    "    # Same forward propagation as before\n",
    "    layer_1_spikes = run_spiking_layer(test_spike_train, w1)\n",
    "    layer_2_spikes = run_spiking_layer(layer_1_spikes, w2)\n",
    "    network_output = torch.sum(layer_2_spikes, 2)  # Count the spikes over time axis\n",
    "\n",
    "    # Do the prediction by selecting the output neuron with the most number of spikes\n",
    "    _, am = torch.max(network_output, 1)\n",
    "    correct_label_count += np.sum(am.detach().cpu().numpy() == y[batch])\n",
    "\n",
    "print(\"Model accuracy on test set: %.3f\" % (correct_label_count / len(test_indices)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Graphiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Évolution des poids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification des changements dans w1\n",
    "w1_history = np.array(w1_history)\n",
    "\n",
    "w1_changes = []  # Liste des indices où les poids de w1 changent\n",
    "for i in range(w1_history.shape[1]):  # Parcours des unités de la couche d'entrée\n",
    "    for j in range(w1_history.shape[2]):  # Parcours des unités de la couche cachée\n",
    "        if w1_history[-1, i, j] != w1_history[0, i, j]:  # Comparaison poids initial et final\n",
    "            w1_changes.append((i, j))\n",
    "\n",
    "# Résultat pour w1\n",
    "if w1_changes:\n",
    "    print(f\"Indices des poids de w1 ayant changé : {w1_changes}\")\n",
    "else:\n",
    "    print(\"Les poids de w1 n'ont pas changé.\")\n",
    "\n",
    "additional_indices = []\n",
    "if len(w1_changes) > 0:\n",
    "    additional_indices = w1_changes[:2]\n",
    "\n",
    "changed_weights = len(w1_changes)\n",
    "\n",
    "# Affichage des résultats\n",
    "total_weights = w1_history.shape[1] * w1_history.shape[2]\n",
    "print(f\"Nombre total de poids dans w1 : {total_weights}\")\n",
    "print(f\"Nombre de poids ayant changé dans w1 : {changed_weights}\")\n",
    "percentage_changed = (changed_weights / total_weights) * 100\n",
    "print(f\"\\nPourcentage de poids ayant changé dans w1 : {percentage_changed:.2f}%\")\n",
    "percentage_not_changed_w1 = 100 - percentage_changed\n",
    "print(f\"Pourcentage de poids n'ayant pas changé dans w1 : {percentage_not_changed_w1:.2f}%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir w2_history en tableau numpy pour faciliter les manipulations\n",
    "w2_history = np.array(w2_history)\n",
    "\n",
    "# Vérification des changements dans w2\n",
    "w2_changes = []  # Liste des indices où les poids de w2 changent\n",
    "for i in range(w2_history.shape[1]):  # Parcours des unités de la couche cachée\n",
    "    for j in range(w2_history.shape[2]):  # Parcours des unités de la couche de sortie\n",
    "        if w2_history[-1, i, j] != w2_history[0, i, j]:  # Comparaison poids initial et final\n",
    "            w2_changes.append((i, j))\n",
    "\n",
    "# Affichage des indices des poids de w2 ayant changé\n",
    "if w2_changes:\n",
    "    print(f\"Indices des poids de w2 ayant changé : {w2_changes}\")\n",
    "else:\n",
    "    print(\"Les poids de w2 n'ont pas changé.\")\n",
    "\n",
    "# Nombre total de poids\n",
    "total_weights_w2 = w2_history.shape[1] * w2_history.shape[2]\n",
    "\n",
    "# Nombre de poids ayant changé\n",
    "changed_weights_w2 = len(w2_changes)\n",
    "\n",
    "# Calcul du pourcentage de poids ayant changé\n",
    "percentage_changed_w2 = (changed_weights_w2 / total_weights_w2) * 100\n",
    "percentage_not_changed_w2 = 100 - percentage_changed_w2\n",
    "\n",
    "# Affichage des résultats pour w2\n",
    "print(f\"\\nNombre total de poids dans w2 : {total_weights_w2}\")\n",
    "print(f\"Nombre de poids ayant changé dans w2 : {changed_weights_w2}\")\n",
    "print(f\"\\nPourcentage de poids ayant changé dans w2 : {percentage_changed_w2:.2f}%\")\n",
    "print(f\"Pourcentage de poids n'ayant pas changé dans w2 : {percentage_not_changed_w2:.2f}%\")\n",
    "\n",
    "# Sélection des deux premiers poids qui ont changé\n",
    "additional_indices_w2 = []\n",
    "if changed_weights_w2 > 0:\n",
    "    additional_indices_w2 = w2_changes[:2]  # Prendre les deux premiers indices ayant changé\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_changed_tot = ((changed_weights_w2 +changed_weights)/ (total_weights_w2+total_weights)) * 100\n",
    "percentage_not_changed_tot = 100 - percentage_changed_tot\n",
    "print(f\"\\nPourcentage de poids ayant changé au total : {percentage_changed_tot:.2f}%\")\n",
    "print(f\"Pourcentage de poids n'ayant pas changé au total : {percentage_not_changed_tot:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir w1_history et w2_history en tableaux numpy pour faciliter les manipulations\n",
    "w1_history = np.array(w1_history)\n",
    "w2_history = np.array(w2_history)\n",
    "print(w1_history.shape)#ombre d'époques, nombre d'unités dans la couche d'entrée,nombre d'unités dans la couche cachée\n",
    "print(w2_history.shape)#ombre d'époques, nombre d'unités dans la couche d'entrée,nombre d'unités dans la couche de sortie\n",
    "\n",
    "# Définir le nombre d'époques\n",
    "n_epochs = w1_history.shape[0]\n",
    "\n",
    "# Indices à tracer pour w1 et w2\n",
    "indices_w1 = [0, 1, 111, 222, 333, -1]  # Indices pour w1\n",
    "for idx in additional_indices:\n",
    "    indices_w1.append(idx[0])  # Ajouter les indices des deux premiers poids ayant changé\n",
    "\n",
    "indices_w2 = [0, 1, 50, 60, 110, -1]  # Indices pour w2\n",
    "\n",
    "# Extraire les poids à ces indices pour w1 et w2\n",
    "w1_weights_to_plot = [w1_history[:, i, 0] for i in indices_w1]\n",
    "w2_weights_to_plot = [w2_history[:, i, 0] for i in indices_w2]\n",
    "\n",
    "# Tracer l'évolution des poids de w1 (shape (5, 784, 128))\n",
    "plt.figure(figsize=(10, 3))\n",
    "\n",
    "for i, weight in enumerate(w1_weights_to_plot):\n",
    "    label = f\"Poids w1[{indices_w1[i]},0]\"\n",
    "    plt.plot(weight, label=label)\n",
    "\n",
    "plt.title(\"Évolution des poids de w1 au fil du temps\")\n",
    "plt.xlabel(\"Époque\")\n",
    "plt.ylabel(\"Valeur du poids\")\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Tracer l'évolution des poids de w2 (shape (5, 128, 10))\n",
    "plt.figure(figsize=(10, 3))\n",
    "\n",
    "for i, weight in enumerate(w2_weights_to_plot):\n",
    "    label = f\"Poids w2[{indices_w2[i]},0]\"\n",
    "    plt.plot(weight, label=label)\n",
    "\n",
    "plt.title(\"Évolution des poids de w2 au fil du temps\")\n",
    "plt.xlabel(\"Époque\")\n",
    "plt.ylabel(\"Valeur du poids\")\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Évolution du loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exemple de dictionnaire contenant les pertes pour chaque fonction\n",
    "\n",
    "\n",
    "# Tracé des courbes\n",
    "plt.figure(figsize=(10, 5))\n",
    "for function_name, losses in loss_history.items():\n",
    "    epochs = range(1, len(losses) + 1)  # Crée un range d'époques correspondant à chaque liste de pertes\n",
    "    plt.plot(epochs, losses, marker='o', label=function_name)\n",
    "    print(f\"{function_name} : {losses:.3f}\")\n",
    "\n",
    "# Personnalisation du graphique\n",
    "plt.title(\"Évolution des pertes en fonction des époques\", fontsize=14)\n",
    "plt.xlabel(\"Époques\", fontsize=12)\n",
    "plt.ylabel(\"Pertes\", fontsize=12)\n",
    "plt.legend(title=\"Fonctions\", fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Affichage\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
